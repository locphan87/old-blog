---
title: AWS Certified Solutions Architect Study Guide Summary
date: 2020-12-24
tags:
  - aws
  - solution-architect
  - study-guide
  - summary
---

## Introduction

This study guide covers the basics of AWS services and concepts within the scope of the AWS Certified Solutions Architect - Associate exam.

| Domain                                                                       | Percentage of Exam |
| ---------------------------------------------------------------------------- | ------------------ |
| Designing highly available, cost-efficient, fault-tolerant, scalable systems | 60                 |
| Implementation/Deployment                                                    | 10                 |
| Data Security                                                                | 20                 |
| Troubleshooting                                                              | 10                 |

<!--
## My Score

| Chapter | Round 1 | Round 2 |
| 0       | 20/25   |
| 1       | 8/10    | 10/10   |
| 2       | 18/20   | 18.5/20 |
| 3       | 14.5/20 | 16.5/20 |
| 4       | 10.5/20 | 17/20   |
| 5       | 13.5/20 | 18.2/20 |
| 6       | 8.2/10  |
| 7       | 15.7/20 |
| 8       | 14/20   |
| 9       | 16/20   |
| 10      | 8.2/10  | 10/10
| 11      | 15/20  |
| 12      | 15/20  |
| 13      | 10/13  |
| 14      | 13/20  |
-->

## Chapter 1: Introduction to AWS

This chapter provides an introduction to the AWS Cloud computing platform. It discusses the advantages of cloud computing and the fundamentals of AWS

### What's Cloud Computing?

- pay-as-you-go pricing
- easy way to access servers, storage, databases and other services

Advantages of cloud computing:

1. Variable vs Capital Expense
1. Economies of Scale
1. Stop Guessing Capacity
1. Increase Speed and Agility
1. Focus on Business Differentiators
1. Go Global in Minutes

Cloud computing deployment models:

- **all-in**: fully deployed, all components running in the cloud
- **hybrid**: between the cloud and existing on-premise infrastructure

### AWS Fundamentals

Using AWS resources instead of your own is like purchasing electricity from a power company instead of running your own generator.

Key advantages of cloud computing:

- capacity exactly matches your need,
- you pay only for what you use,
- economies of scale result in lower costs,
- and the service is provided by a vendor experienced in running large-scale networks.

Key foundational concepts:

- AWS global infrastructure
- AWS approach to security and compliance

### AWS Cloud Computing Platform

_Figure 1.2: AWS Cloud computing platform_
![AWS Cloud computing platform](https://learning.oreilly.com/library/view/aws-certified-solutions/9781119138556/images/ec01f002.jpg)

Accessing the platform

- AWS Management Console
- AWS CLI
- AWS SDK

Compute and networking services

- Amazon Elastic Compute Cloud (Amazon EC2)
- AWS Lambda
- Auto Scaling
- Elastic Load Balancing
- AWS Elastic Beanstalk
- Amazon Virtual Private Cloud (Amazon VPC)
- AWS Direct Connect
- Amazon Route 53

Storage and content delivery

- Amazon Simple Storage Service (Amazon S3)
- Amazon Glacier
- Amazon Elastic Block Store (Amazon EBS)
- AWS Storage Gateway
- Amazon CloudFront

Database services

- Amazon Relational Database Service (Amazon RDS)
- Amazon DynamoDB
- Amazon Redshift
- Amazon ElastiCache

Management tools

- Amazon CloudWatch
- AWS CloudFormation

_Figure 1.4: AWS CloudFormation workflow summary_
![AWS CloudFormation workflow summary](https://learning.oreilly.com/library/view/aws-certified-solutions/9781119138556/images/ec01f004.jpg)

- AWS CloudTrail
- AWS Config

Security and identity

- AWS Identity and Access Management (IAM)
- AWS Key Management Service (KMS)
- AWS Directory Service
- AWS Certificate Manager
- AWS Web Application Firewall (WAF)

Application services

- Amazon API Gateway
- Amazon Elastic Transcoder
- Amazon Simple Notification Service (Amazon SNS)
- Amazon Simple Workflow Service (Amazon SWF)
- Amazon Simple Queue Service (Amazon SQS)

### Summary

The term “cloud computing” refers to the on-demand delivery of IT resources via the Internet with pay-as-you-go pricing. Instead of buying, owning, and maintaining data centers and servers, organizations can acquire technology such as compute power, storage, databases, and other services on an as-needed basis. With cloud computing, AWS manages and maintains the technology infrastructure in a secure environment and businesses access these resources via the Internet to develop and run their applications. Capacity can grow or shrink instantly and businesses pay only for what they use.

Cloud computing introduces a revolutionary shift in how technology is obtained, used, and managed, and how organizations budget and pay for technology services. While each organization experiences a unique journey to the cloud with numerous benefits, six advantages become apparent time and time again. Understanding these advantages allows architects to shape solutions that deliver continuous benefits to organizations.

AWS provides a highly available technology infrastructure platform with multiple locations worldwide. These locations are composed of regions and Availability Zones. This enables organizations to place resources and data in multiple locations around the globe. Helping to protect the confidentiality, integrity, and availability of systems and data is of the utmost importance to AWS, as is maintaining the trust and confidence of organizations around the world.

AWS offers a broad set of global compute, storage, database, analytics, application, and deployment services that help organizations move faster, lower IT costs, and scale applications. Having a broad understanding of these services allows solutions architects to design effective distributed applications and systems on the AWS platform.

### Exam Essentials

**Understand the global infrastructure.**
Highly available infrastructure with multiple locations. Locations = regions + Availability Zones

**Understand regions.**
independent, isolated from the other regions. Resources aren't replicated across regions

**Understand Availability Zones.**
data centers, isolated from failures in other Availability Zones

**Understand the hybrid deployment model.**
between cloud-based resources and existing resources that are not located in the cloud

### Review Questions

- physical location
- Availabiljity Zones
- CloudWatch
- Auto Scaling
- CloudFront
- EBS
- VPC
- SQS

## Chapter 2: Amazon Simple Storage Service (Amazon S3) and Amazon Glacier Storage

### Summary

Amazon S3 is the core object storage service on AWS, allowing you to store an unlimited amount of data with very high durability.

Common Amazon S3 use cases include backup and archive, web content, big data analytics, static website hosting, mobile and cloud-native application hosting, and disaster recovery.

Amazon S3 is integrated with many other AWS cloud services, including AWS IAM, AWS KMS, Amazon EC2, Amazon EBS, Amazon EMR, Amazon DynamoDB, Amazon Redshift, Amazon SQS, AWS Lambda, and Amazon CloudFront.

Object storage differs from traditional block and file storage. Block storage manages data at a device level as addressable blocks, while file storage manages data at the operating system level as files and folders. Object storage manages data as objects that contain both data and metadata, manipulated by an API.

Amazon S3 buckets are containers for objects stored in Amazon S3. Bucket names must be globally unique. Each bucket is created in a specific region, and data does not leave the region unless explicitly copied by the user.

Amazon S3 objects are files stored in buckets. Objects can be up to 5TB and can contain any kind of data. Objects contain both data and metadata and are identified by keys. Each Amazon S3 object can be addressed by a unique URL formed by the web services endpoint, the bucket name, and the object key.

Amazon S3 has a minimalistic API—create/delete a bucket, read/write/delete objects, list keys in a bucket—and uses a REST interface based on standard HTTP verbs—GET, PUT, POST, and DELETE. You can also use SDK wrapper libraries, the AWS CLI, and the AWS Management Console to work with Amazon S3.

Amazon S3 is highly durable and highly available, designed for 11 nines of durability of objects in a given year and four nines of availability.

Amazon S3 is eventually consistent, but offers read-after-write consistency for new object PUTs.

Amazon S3 objects are private by default, accessible only to the owner. Objects can be marked public readable to make them accessible on the web. Controlled access may be provided to others using ACLs and AWS IAM and Amazon S3 bucket policies.

Static websites can be hosted in an Amazon S3 bucket.

Prefixes and delimiters may be used in key names to organize and navigate data hierarchically much like a traditional file system.

Amazon S3 offers several storage classes suited to different use cases: Standard is designed for general-purpose data needing high performance and low latency. Standard-IA is for less frequently accessed data. RRS offers lower redundancy at lower cost for easily reproduced data. Amazon Glacier offers low-cost durable storage for archive and long-term backups that can are rarely accessed and can accept a three- to five-hour retrieval time.

Object lifecycle management policies can be used to automatically move data between storage classes based on time.

Amazon S3 data can be encrypted using server-side or client-side encryption, and encryption keys can be managed with Amazon KMS.

Versioning and MFA Delete can be used to protect against accidental deletion.

Cross-region replication can be used to automatically copy new objects from a source bucket in one region to a target bucket in another region.

Pre-signed URLs grant time-limited permission to download objects and can be used to protect media and other web content from unauthorized “web scraping.”

Multipart upload can be used to upload large objects, and Range GETs can be used to download portions of an Amazon S3 object or Amazon Glacier archive.

Server access logs can be enabled on a bucket to track requestor, object, action, and response.

Amazon S3 event notifications can be used to send an Amazon SQS or Amazon SNS message or to trigger an AWS Lambda function when an object is created or deleted.

Amazon Glacier can be used as a standalone service or as a storage class in Amazon S3.

Amazon Glacier stores data in archives, which are contained in vaults. You can have up to 1,000 vaults, and each vault can store an unlimited number of archives.

Amazon Glacier vaults can be locked for compliance purposes.

### Exam Essentials

**Know what amazon S3 is and what it is commonly used for.**
secure, durable, and highly scalable cloud storage.

Common use cases include

- backup and archive
- content storage and distribution
- big data analytics
- static website hosting
- cloud-native application hosting
- disaster recovery

**Understand how object storage differs from block and file storage.**

- **Cloud object** storage manages data at the application level as objects using a REST API built on HTTP.
- **Block** storage manages data at the operating system level as numbered addressable blocks.
- **File** storage manages data as shared files.

**Understand the basics of Amazon S3.**
S3 stores data in objects that contain data and metadata. Objects are identified by a user-defined key and are stored in a simple flat folder called a bucket

**Understand the durability, availability, and data consistency model of Amazon S3.**
Amazon S3 standard storage is designed for **11 nines durability** and **four nines availability** of objects over a year. S3 is eventually consistent, but offers read-after-write consistency for PUTs to new objects.

**Know how to enable static website hosting on Amazon S3.**

- create a bucket with the website hostname
- upload your static content and make it public
- enable static website hosting on the bucket
- indicate the index and error page objects

**Know how to protect your data on Amazon S3.**

- Encrypt data in flight using HTTPS and at rest using SSE or client-side encryption.
- Enable versioning to keep multiple versions of an object in a bucket.
- Enable MFA Delete to protect against accidental deletion.
- Use ACLs Amazon S3 bucket policies and AWS IAM policies for access control.
- Use pre-signed URLs for time-limited download access.
- Use cross-region replication to automatically replicate data to another region.

**Know the use case for each of the Amazon S3 storage classes.**

- **Standard** - high durability, high performance, and low latency access.
- **Standard-IA** - less frequently accessed, but that needs the same performance and availability when accessed.
- **RRS** - lower durability at lower cost for easily replicated data.
- **Glacier** - storing rarely accessed archival data at lowest cost, when three- to five-hour retrieval time is acceptable.

**Know how to use lifecycle configuration rules.**
Lifecycle configuration rules define actions to transition objects from one storage class to another based on time.

**Know how to use Amazon S3 event notifications.**
set at the bucket level and can trigger a message in Amazon SNS or Amazon SQS or an action in AWS Lambda in response to an upload or a delete of an object.

**Know the basics of amazon glacier as a standalone service.**
Data is stored in encrypted archives that can be as large as 40TB. Archives typically contain TAR or ZIP files. Vaults are containers for archives, and vaults can be locked for compliance.

### Exercises

| ID  | Exercise                                                       |
| --- | -------------------------------------------------------------- |
| 2.1 | Create an Amazon Simple Storage Service (Amazon S3) Bucket     |
| 2.2 | Upload, Make Public, Rename, and Delete Objects in Your Bucket |
| 2.3 | Enable Version Control                                         |
| 2.4 | Delete an Object and Then Restore It                           |
| 2.5 | Lifecycle Management                                           |
| 2.6 | Enable Static Hosting on Your Bucket                           |

References:

1. [Getting started with Amazon S3](http://docs.aws.amazon.com/AmazonS3/latest/gsg/GetStartedWithS3.html)
1. [Set up a static website](http://docs.aws.amazon.com/AmazonS3/latest/dev/HostingWebsiteOnS3Setup.html)
1. [Using versioning](http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html)
1. [Object Lifecycle Management](http://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html)

### Review Questions

- object vs block vs file storage
- MFA Delete
- use cases of S3
- characteristics of S3
- restrict access to S3
- protected against inadvertent or intentional deletion
- encryption
- enable cross-region replication
- storage plan

## Chapter 3: Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Elastic Block Store (Amazon EBS)

### Summary

Compute is the amount of computational power required to fulfill your workload. Amazon EC2 is the primary service for providing compute to customers.

The instance type defines the virtual hardware supporting the instance. Available instance types vary in vCPUs, memory, storage, and network performance to address nearly any workload.

An AMI defines the initial software state of the instance, both OS and applications. There are four sources of AMIs: AWS published generic OSs, partner-published AMIs in the AWS Marketplace with software packages preinstalled, customer-generated AMIs from existing Amazon EC2 instances, and uploaded AMIs from virtual servers.

Instances can be addressed by public DNS name, public IP address, or elastic IP address. To access a newly launched Linux instance, use the private half of the key pair to connect to the instance via SSH. To access a newly created Windows instance, use the private half of the key pair to decrypt the randomly initialized local administrator password.

Network traffic in and out of an instance can be controlled by a virtual firewall called a security group. A security group allows rules that block traffic based on direction, port, protocol, and source/destination address.

Bootstrapping allows you to run a script to initialize your instance with OS configurations and applications. This feature allows instances to configure themselves upon launch. Once an instance is launched, you can change its instance type or, for Amazon VPC instances, the security groups with which it is associated.

The three pricing options for instances are On-Demand, Reserved Instance, and Spot. On-Demand has the highest per hour cost, requiring no up-front commitment and giving you complete control over the lifetime of the instance. Reserved Instances require a commitment and provide a reduced overall cost over the lifetime of the reservation. Spot Instances are idle compute capacity that AWS makes available based on bid prices from customers. The savings on the per-hour cost can be significant, but instances can be shut down when the bid price exceeds the customer’s current bid.

Instance stores are block storage included with the hourly cost of the instance. The amount and type of storage available varies with the instance type. Instance stores terminate when the associated instance is stopped, so they should only be used for temporary data or in architectures providing redundancy such as Hadoop’s HDFS.

Amazon EBS provides durable block storage in several types. Magnetic has the lowest cost per gigabyte and delivers modest performance. General-purpose SSD is cost-effective storage that can provide up to 10,000 IOPS. Provisioned IOPS SSD has the highest cost per gigabyte and is well suited for I/O-intensive workloads sensitive to storage performance. Snapshots are incremental backups of Amazon EBS volumes stored in Amazon S3. Amazon EBS volumes can be encrypted.

### Exam Essentials

**Know the basics of launching an Amazon EC2 instance.**
To launch an instance, you must specify an **AMI**, which defines the software on the instance at launch, and an **instance type**, which defines the virtual hardware supporting the instance (memory, vCPUs, and so on).

**Know what architectures are suited for what Amazon EC2 pricing options.**

- **Spot** Instances are best suited for workloads that can accommodate interruption.
- **Reserved** Instances are best for consistent, long-term compute needs.
- **On-Demand** Instances provide flexible compute to respond to scaling needs.

**Know how to combine multiple pricing options that result in cost optimization and scalability.**
On-Demand Instances can be used to scale up a web application running on Reserved Instances in response to a temporary traffic spike. For a workload with several Reserved Instances reading from a queue, it’s possible to use Spot Instances to alleviate heavy traffic in a cost-effective way.

**Know the benefits of enhanced networking.**
Enhanced networking enables you to get significantly higher PPS performance, lower network jitter, and lower latencies.

**Know the capabilities of VM import/export.**
VM Import/Export allows you to import existing VMs to AWS as Amazon EC2 instances or AMIs. Amazon EC2 instances that were imported through VM Import/Export can also be exported back to a virtual environment.

**Know the methods for accessing an instance over the internet.**
You can access an Amazon EC2 instance over the web via public IP address, elastic IP address, or public DNS name.
There are additional ways to access an instance within an Amazon VPC, including private IP addresses and ENIs.

**Know the lifetime of an instance store.**
Data on an instance store is lost when the instance is stopped or terminated. Instance store data survives an OS reboot.

**Know the properties of the Amazon EC2 pricing options.**

- **On-Demand** Instances require no up-front commitment, can be launched any time, and are billed by the hour.
- **Reserved** Instances require an up-front commitment and vary in cost depending on whether they are paid all up front, partially up front, or not up front.
- **Spot** Instances are launched when your bid price exceeds the current spot price. Spot Instances will run until the spot price exceeds your bid price, in which case the instance will get a two-minute warning and terminate.

**Know what determines network performance.**
Every instance type is rated for low, moderate, high, or 10 Gbps network performance, with larger instance types generally having higher ratings. Additionally, some instance types offer enhanced networking, which provides additional improvement in network performance.

**Know what instance metadata is and how it’s obtained.**
Metadata is information about an Amazon EC2 instance, such as instance ID, instance type, and security groups, that is available from within the instance. It can be obtained through an HTTP call to a specific IP address.

**Know how security groups protect instances.**
Security groups are virtual firewalls controlling traffic in and out of your Amazon EC2 instances. They are deny by default, and you can allow traffic by adding rules specifying traffic direction, port, protocol, and destination address (via Classless Inter-Domain Routing [CIDR] block). They are applied at the instance level, meaning that traffic between instances in the same security group must adhere to the rules of that security group. They are stateful, meaning that an outgoing rule will allow the response without a correlating incoming rule.

**Know how to interpret the effect of security groups.**
When an instance is a member of multiple security groups, the effect is a union of all the rules in all the groups.

**Know the different Amazon EBS volume types, their characteristics, and their appropriate workloads.**

- **Magnetic** volumes provide an average performance of 100 IOPS and can be provisioned up to 1 TB. They are good for cold and infrequently accessed data.
- **General-purpose** SSD volumes provide three IOPS/GB up to 10,000 IOPS, with smaller volumes able to burst 3,000 IOPS. They can be provisioned up to 16 TB and are appropriate for dev/test environments, small databases, and so forth.
- **Provisioned** IOPS SSD can provide up to 20,000 consistent IOPS for volumes up to 16 TB. They are the best choice for workloads such as large databases executing many transactions.

**Know how to encrypt an Amazon EBS volume.**
Any volume type can be encrypted at launch. Encryption is based on AWS KMS and is transparent to applications on the attached instances.

**Understand the concept and process of snapshots.**
Snapshots provide a point-in-time backup of an Amazon EBS volume and are stored in Amazon S3. Subsequent snapshots are incremental—they only store deltas. When you request a snapshot, the point-in-time snapshot is created immediately and the volume may continue to be used, but the snapshot may remain in pending status until all the modified blocks have been transferred to Amazon S3. Snapshots may be copied between regions.

**Know how Amazon EBS-optimized instances affect Amazon EBS performance.**
In addition to the IOPS that control the performance in and out of the Amazon EBS volume, use Amazon EBS-optimized instances to ensure additional, dedicated capacity for Amazon EBS I/O.

### Exercises

| ID  | Exercise                                                                              |
| --- | ------------------------------------------------------------------------------------- |
| 3.1 | Launch and Connect to a Linux Instance                                                |
| 3.2 | Launch a Windows Instance with Bootstrapping                                          |
| 3.3 | Confirm That Instance Stores Are Lost When an Instance Is Stopped                     |
| 3.4 | Launch a Spot Instance                                                                |
| 3.5 | Access Metadata                                                                       |
| 3.6 | Create an Amazon EBS Volume and Show That It Remains After the Instance Is Terminated |
| 3.7 | Take a Snapshot and Restore                                                           |
| 3.8 | Launch an Encrypted Volume                                                            |
| 3.9 | Detach a Boot Drive and Reattach to Another Instance                                  |

References

1. [Amazon EC2 Linux](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html)
1. [Amazon EC2 Windows](http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/concepts.html)
1. [Amazon EBS](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html)

### Review questions

- cost-effective way to handle a traffic pattern
- launch EC2
- modify an instance
- effect of security groups
- features of enhanced networking
- dedicated instances
- placement group
- instance stores
- features of EBS
- snapshots
- bootstrapping
- VM import/export
- access an instance

## Chapter 4: Amazon Virtual Private Cloud (Amazon VPC)

### Summary

In this chapter, you learned that Amazon VPC is the networking layer for Amazon EC2, and it allows you to create your own private virtual network within the cloud. You can provision your own logically isolated section of AWS similar to designing and implementing a separate independent network that you’d operate in a physical data center.

A VPC consists of the following components:

- Subnets
- Route tables
- DHCP option sets
- Security groups
- Network ACLs

A VPC has the following optional components:

- IGWs
- EIP addresses
- Endpoints
- Peering
- NAT instance and NAT gateway
- VPG, CGW, and VPN

Subnets can be public, private, or VPN-only. A public subnet is one in which the associated route table directs the subnet’s traffic to the Amazon VPC’s IGW. A private subnet is one in which the associated route table does not direct the subnet’s traffic to the Amazon VPC’s IGW. A VPN-only subnet is one in which the associated route table directs the subnet’s traffic to the Amazon VPC’s VPG and does not have a route to the IGW. Regardless of the type of subnet, the internal IP address range of the subnet is always private (non-routable on the Internet).

A route table is a logical construct within an Amazon VPC that contains a set of rules (called routes) that are applied to the subnet and used to determine where network traffic is directed. A route table’s routes are what permit Amazon EC2 instances within different subnets within an Amazon VPC to communicate with each other. You can modify route tables and add your own custom routes. You can also use route tables to specify which subnets are public (by directing Internet traffic to the IGW) and which subnets are private (by not having a route that directs traffic to the IGW). An IGW is a horizontally scaled, redundant, and highly available Amazon VPC component that allows communication between instances in your Amazon VPC and the Internet. IGWs are fully redundant and have no bandwidth constraints. An IGW provides a target in your Amazon VPC route tables for Internet-routable traffic, and it performs network address translation for instances that have been assigned public IP addresses.

The DHCP option sets element of an Amazon VPC allows you to direct Amazon EC2 host name assignment to your own resources. In order for you to assign your own domain name to your instances, you create a custom DHCP option set and assign it to your Amazon VPC.

An EIP address is a static, public IP address in the pool for the region that you can allocate to your account (pull from the pool) and release (return to the pool). EIPs allow you to maintain a set of IP addresses that remain fixed while the underlying infrastructure may change over time.

An Amazon VPC endpoint enables you to create a private connection between your Amazon VPC and another AWS service without requiring access over the Internet or through a NAT instance, VPN connection, or AWS Direct Connect. You can create multiple endpoints for a single service, and you can use different route tables to enforce different access policies from different subnets to the same service.

An Amazon VPC peering connection is a networking connection between two Amazon VPCs that enables instances in either Amazon VPC to communicate with each other as if they were within the same network. You can create an Amazon VPC peering connection between your own Amazon VPCs or with an Amazon VPC in another AWS account within a single region. A peering connection is neither a gateway nor a VPN connection and does not introduce a single point of failure for communication.

A security group is a virtual stateful firewall that controls inbound and outbound traffic to Amazon EC2 instances. When you first launch an Amazon EC2 instance into an Amazon VPC, you must specify the security group with which it will be associated. AWS provides a default security group for your use, which has rules that allow all instances associated with the security group to communicate with each other and allow all outbound traffic. You may change the rules for the default security group, but you may not delete the default security group.

A network ACL is another layer of security that acts as a stateless firewall on a subnet level. Amazon VPCs are created with a modifiable default network ACL associated with every subnet that allows all inbound and outbound traffic. If you want to create a custom network ACL, its initial configuration will deny all inbound and outbound traffic until you create a rule that states otherwise.

A NAT instance is a customer-managed instance that is designed to accept traffic from instances within a private subnet, translate the source IP address to the public IP address of the NAT instance, and forward the traffic to the IGW. In addition, the NAT instance maintains the state of the forwarded traffic in order to return response traffic from the Internet to the proper instance in the private subnet.

A NAT gateway is an AWS-managed service that is designed to accept traffic from instances within a private subnet, translate the source IP address to the public IP address of the NAT gateway, and forward the traffic to the IGW. In addition, the NAT gateway maintains the state of the forwarded traffic in order to return response traffic from the Internet to the proper instance in the private subnet.

A VPG is the VPN concentrator on the AWS side of the VPN connection between the two networks. A CGW is a physical device or a software application on the customer’s side of the VPN connection. After these two elements of an Amazon VPC have been created, the last step is to create a VPN tunnel. The VPN tunnel is established after traffic is generated from the customer’s side of the VPN connection.

### Exam Essentials

**Understand what a VPC is and its core and optional components.**
An Amazon VPC is a logically isolated network in the AWS Cloud.

An Amazon VPC is made up of the following core elements: subnets (public, private, and VPN-only), route tables, DHCP option sets, security groups, and network ACLs.

Optional elements include an IGW, EIP addresses, endpoints, peering connections, NAT instances, VPGs, CGWs, and VPN connections.

**Understand the purpose of a subnet.**
A subnet is a segment of an Amazon VPC’s IP address range where you can place groups of isolated resources. Subnets are defined by CIDR blocks—for example, 10.0.1.0/24 and 10.0.2.0/24—and are contained within an Availability Zone.

**Identify the difference between a public subnet, a private subnet, and a VPN-Only subnet.**

- If a subnet’s traffic is routed to an IGW, the subnet is known as a **public** subnet.
- If a subnet doesn’t have a route to the IGW, the subnet is known as a **private** subnet.
- If a subnet doesn’t have a route to the IGW, but has its traffic routed to a VPG, the subnet is known as a **VPN-only** subnet.

**Understand the purpose of a route table.**
A route table is a set of rules (called routes) that are used to determine where network traffic is directed. A route table allows Amazon EC2 instances within different subnets to communicate with each other (within the same Amazon VPC). The Amazon VPC router also enables subnets, IGWs, and VPGs to communicate with each other.

**Understand the purpose of an IGW.**
An IGW is a horizontally scaled, redundant, and highly available Amazon VPC component that allows communication between instances in your Amazon VPC and the Internet. IGWs are fully redundant and have no bandwidth constraints. An IGW provides a target in your Amazon VPC route tables for Internet-routable traffic and performs network address translation for instances that have been assigned public IP addresses.

**Understand what DHCP option sets provide to an Amazon VPC.**
The DHCP option sets element of an Amazon VPC allows you to direct Amazon EC2 host name assignment to your own resources. You can specify the domain name for instances within an Amazon VPC and identify the IP addresses of custom DNS servers, NTP servers, and NetBIOS servers.

**Know the difference between an Amazon VPC public IP address and an EIP address.**

- A public IP address is an AWS-owned IP that can be automatically assigned to instances launched within a subnet.
- An EIP address is an AWS-owned public IP address that you allocate to your account and assign to instances or network interfaces on demand.

**Understand what endpoints provide to an Amazon VPC.**
An Amazon VPC endpoint enables you to create a private connection between your Amazon VPC and another AWS service without requiring access over the Internet or through a NAT instance, a VPN connection, or AWS Direct Connect. Endpoints support services within the region only.

**Understand Amazon VPC peering.**
An Amazon VPC peering connection is a networking connection between two Amazon VPCs that enables instances in either Amazon VPC to communicate with each other as if they are within the same network. Peering connections are created through a request/accept protocol. Transitive peering is not supported, and peering is only available between Amazon VPCs within the same region.

**Know the difference between a security group and a network ACL.**
A security group applies at the **instance level**. You can have multiple instances in multiple subnets that are members of the same security groups. Security groups are **stateful**, which means that return traffic is automatically allowed, regardless of any outbound rules. A network ACL is applied on a **subnet level**, and traffic is **stateless**. You need to allow both inbound and outbound traffic on the network ACL in order for Amazon EC2 instances in a subnet to be able to communicate over a particular protocol.

**Understand what a NAT provides to an Amazon VPC.**
A NAT instance or NAT gateway enables instances in a private subnet to initiate outbound traffic to the Internet. This allows outbound Internet communication to download patches and updates, for example, but prevents the instances from receiving inbound traffic initiated by someone on the Internet.

**Understand the components needed to establish a VPN connection from a network to an Amazon VPC.**
A VPG is the VPN concentrator on the AWS side of the VPN connection between the two networks. A CGW represents a physical device or a software application on the customer’s side of the VPN connection. The VPN connection must be initiated from the CGW side, and the connection consists of two IPSec tunnels.

### Exercises

1. Create a Custom Amazon VPC
1. Create Two Subnets for Your Custom Amazon VPC
1. Connect Your Custom Amazon VPC to the Internet and Establish Routing
1. Launch an Amazon EC2 Instance and Test the Connection to the Internet

References:

- [Amazon VPC User Guide](http://aws.amazon.com/documentation/vpc/)

### Review Questions

- minimum/maximum size subnet
- numbers of IGW's to attach to an Amazon VPC
- peering connections
- default limit for the number of Amazon VPCs that a customer may have in a region
- VPG, CGW, IGW
- VPC endpoint

## Chapter 5: Elastic Load Balancing, Amazon CloudWatch, and Auto Scaling

### Summary

This chapter introduced three services:

- Elastic Load Balancing, which is used to distribute traffic across a group of Amazon EC2 instances in one or more Availability Zones to achieve greater levels of fault tolerance for your applications.
- Amazon CloudWatch, which monitors resources and applications. Amazon CloudWatch is used to collect and track metrics, create alarms that send notifications, and make changes to resources being monitored based on rules you define.
- Auto Scaling, which allows you to automatically scale your Amazon EC2 capacity out and in using criteria that you define.

These three services can be used very effectively together to create a highly available application with a resilient architecture on AWS.

### Exam Essentials

**Understand what the Elastic Load Balancing service provides.**
Elastic Load Balancing is a highly available service that distributes traffic across Amazon EC2 instances and includes options that provide flexibility and control of incoming requests to Amazon EC2 instances.

**Know the types of load balancers the Elastic Load Balancing service provides and when to use each one.**
An **Internet-facing** load balancer is, as the name implies, a load balancer that takes requests from clients over the Internet and distributes them to Amazon EC2 instances that are registered with the load balancer.

An **internal** load balancer is used to route traffic to your Amazon EC2 instances in VPCs with private subnets.

An **HTTPS** load balancer is used when you want to encrypt data between your load balancer and the clients that initiate HTTPS sessions and for connections between your load balancer and your back-end instances.

**Know the types of listeners the Elastic Load Balancing service provides and the use case and requirements for using each one.**
A listener is a process that checks for connection requests. It is configured with a protocol and a port for front-end (client to load balancer) connections and a protocol and a port for back-end (load balancer to back-end instance) connections.

**Understand the configuration options for Elastic Load Balancing.**
Elastic Load Balancing allows you to configure many aspects of the load balancer, including

- idle connection timeout,
- cross-zone load balancing,
- connection draining,
- proxy protocol,
- sticky sessions,
- and health checks.

**Know what an Elastic Load Balancing health check is and why it is important.**
Elastic Load Balancing supports health checks to test the status of the Amazon EC2 instances behind an Elastic Load Balancing load balancer.

**Understand what the amazon CloudWatch service provides and what use cases there are for using it.**
Amazon CloudWatch is a service that you can use to

- monitor your AWS resources and your applications in real time
- collect and track metrics
- create alarms that send notifications
- and make changes to the resources being monitored based on rules you define

**Know the differences between the two types of monitoring—basic and detailed—for Amazon CloudWatch.**
**Basic** monitoring sends data points to Amazon CloudWatch every five minutes for a limited number of preselected metrics at no charge.

**Detailed** monitoring sends data points to Amazon CloudWatch every minute and allows data aggregation for an additional charge. If you want to use detailed monitoring, you must enable it—basic is the default.

**Understand Auto Scaling and why it is an important advantage of the AWS Cloud.**
A distinct advantage of deploying applications to the cloud is the ability to launch and then release servers in response to variable workloads. Provisioning servers on demand and then releasing them when they are no longer needed can provide significant **cost savings** for workloads that are not steady state.

**Know when and why to use Auto Scaling.**
Auto Scaling is a service that allows you to scale your Amazon EC2 capacity automatically by scaling out and scaling in according to criteria that you define. With Auto Scaling, you can ensure that the number of running Amazon EC2 instances increases during demand spikes or peak demand periods to maintain application performance and decreases automatically during demand lulls or troughs to minimize costs.

**Know the supported Auto Scaling plans.**
Auto Scaling has several schemes or plans that you can use to control how you want Auto Scaling to perform. The Auto Scaling plans are named

- Maintain Current Instant Levels,
- Manual Scaling,
- Scheduled Scaling,
- and Dynamic Scaling.

**Understand how to build an Auto Scaling launch configuration and an Auto Scaling group and what each is used for.**
A launch configuration is the **template** that Auto Scaling uses to create new instances and is composed of the configuration name, AMI, Amazon EC2 instance type, security group, and instance key pair.

**Know what a scaling policy is and what use cases to use it for.**
A scaling policy is used by Auto Scaling with CloudWatch alarms to **determine when** your Auto Scaling group should scale out or scale in. Each CloudWatch alarm watches a single metric and sends messages to Auto Scaling when the metric breaches a threshold that you specify in your policy.

**Understand how Elastic Load Balancing, amazon CloudWatch, and Auto Scaling are used together to provide dynamic scaling.**
Elastic Load Balancing, Amazon CloudWatch, and Auto Scaling can be used together to create a highly available application with a resilient architecture on AWS.

### Exercises

1. Create an Elastic Load Balancing Load Balancer
1. Use an Amazon CloudWatch Metric
1. Create a Custom Amazon CloudWatch Metric
1. Create a Launch Configuration and Auto Scaling Group
1. Create a Scaling Policy
1. Create a Web Application That Scales

References

- [Elastic Load Balancing Developer Guide](http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/elastic-load-balancing.html)
- [Amazon CloudWatch Developer Guide](http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/WhatIsCloudWatch.html)
- [Auto Scaling User Guide](http://docs.aws.amazon.com/autoscaling/latest/userguide/WhatIsAutoScaling.html)

### Review Questions

- elements of an Auto Scaling group
- CloudWatch keep metric data
- create an Auto Scaling launch configuration
- default Amazon EC2 capacity (20) for the new region
- health check
- characteristics of the Auto Scaling service
- instance types an Auto Scaling group may use
- monitoring plans
- types of load balancers
- plans for Auto Scaling groups

## Chapter 6: AWS Identity and Access Management (IAM)

### Summary

IAM is a powerful service that gives you the ability to control which people and applications can access your AWS account at a very granular level. Because the root user in an AWS account cannot be limited, you should set up IAM users and temporary security tokens for your people and processes to interact with AWS.

Policies define what actions can and cannot be taken. Policies are associated with IAM users either directly or through group membership. A temporary security token is associated with a policy by assuming an IAM role. You can write your own policies or use one of the managed policies provided by AWS.

Common use cases for IAM roles include federating identities from external IdPs, assigning privileges to an Amazon EC2 instance where they can be assumed by applications running on the instance, and cross-account access.

IAM user accounts can be further secured by rotating keys, implementing MFA, and adding conditions to policies. MFA ensures that authentication is based on something you have in addition to something you know, and conditions can add further restrictions such as limiting client IP address ranges or setting a particular time interval.

### Exam Essentials

**Know the different principals in IAM.**
The three principals that can authenticate and interact with AWS resources are the root user, IAM users, and roles.

- The root user is associated with the actual AWS account and cannot be restricted in any way.
- IAM users are persistent identities that can be controlled through IAM.
- Roles allow people or processes the ability to operate temporarily with a different identity. People or processes assume a role by being granted a temporary security token that will expire after a specified period of time.

**Know how principals are authenticated in IAM.**
When you log in to the AWS Management Console as an IAM user or root user, you use a user name/password combination. A program that accesses the API with an IAM user or root user uses a two-part access key. A temporary security token authenticates with an access key plus an additional session token unique to that temporary security token.

**Know the parts of a policy.**
A policy is a JSON document that defines one or more permissions to interact with AWS resources. Each permission includes the effect, service, action, and resource. It may also include one or more conditions. AWS makes many predefined policies available as managed policies.

**Know how a policy is associated with a principal.**
An authenticated principal is associated with zero to many policies. For an IAM user, these policies may be attached directly to the user account or attached to an IAM group of which the user account is a member. A temporary security token is associated with policies by assuming an IAM role.

**Understand MFA.**
MFA increases the security of an AWS account by augmenting the password (something you know) with a rotating OTP from a small device (something you have), ensuring that anyone authenticating the account has both knowledge of the password and possession of the device. AWS supports both Gemalto hardware MFA devices and a number of virtual MFA apps.

**Understand key rotation.**
To protect your AWS infrastructure, access keys should be rotated regularly. AWS allows two access keys to be valid simultaneously to make the rotation process straightforward: Generate a new access key, configure your application to use the new access key, test, disable the original access key, test, delete the original access key, and test again.

**Understand IAM roles and federation.**
IAM roles are prepackaged sets of permissions that have no credentials. Principals can assume a role and then use the associated permissions. When a temporary security token is created, it assumes a role that defines the permissions assigned to the token. When an Amazon EC2 instance is associated with an IAM role, SDK calls acquire a temporary security token based on the role associated with the instance and use that token to access AWS resources.

**Roles are the basis for federating external IdPs with AWS.**
You configure an IAM IdP to interact with the external IdP, the authenticated identity from the IdP is mapped to a role, and a temporary security token is returned that has assumed that role. AWS supports both SAML and OIDC IdPs.

**Know how to resolve conflicting permissions.**
Resolving multiple permissions is relatively straightforward. If an action on a resource has not been explicitly allowed by a policy, it is denied. If two policies contradict each other; that is, if one policy allows an action on a resource and another policy denies that action, the action is denied. While this sounds improbable, it may occur due to scope differences in a policy. One policy may expose an entire fleet of Amazon EC2 instances, and a second policy may explicitly lock down one particular instance.

### Exercises

1. Create an IAM Group
1. Create a Customized Sign-In Link and Password Policy
1. Create an IAM User
1. Create and Use an IAM Role
1. Rotate Keys
1. Set Up MFA
1. Resolve Conflicting Permissions

References

- [IAM User Guide](http://docs.aws.amazon.com/IAM/latest/UserGuide/)

### Review Questions

- IAM policy
- authorized actions by IAM
- parts of a policy
- IAM group
- temporary security tokens
- benefits of using Amazon EC2 roles
- IAM security features

## Chapter 7: Databases and AWS

### Summary

In this chapter, you learned the basic concepts of relational databases, data warehouses, and NoSQL databases. You also learned about the benefits and features of AWS managed database services Amazon RDS, Amazon Redshift, and Amazon DynamoDB.

Amazon RDS manages the heavy lifting involved in administering a database infrastructure and software and lets you focus on building the relational schemas that best fit your use case and the performance tuning to optimize your queries.

Amazon RDS supports popular open-source and commercial database engines and provides a consistent operational model for common administrative tasks. Increase your availability by running a master-slave configuration across Availability Zones using Multi-AZ deployment. Scale your application and increase your database read performance using read replicas.

Amazon Redshift allows you to deploy a data warehouse cluster that is optimized for analytics and reporting workloads within minutes. Amazon Redshift distributes your records using columnar storage and parallelizes your query execution across multiple compute nodes to deliver fast query performance. Amazon Redshift clusters can be scaled up or down to support large, petabyte-scale databases using SSD or magnetic disk storage.

Connect to Amazon Redshift clusters using standard SQL clients with JDBC/ODBC drivers and execute SQL queries using many of the same analytics and ETL tools that you use today. Load data into your Amazon Redshift clusters using the COPY command to bulk import flat files stored in Amazon S3, then run standard SELECT commands to search and query the table.

Back up both your Amazon RDS databases and Amazon Redshift clusters using automated and manual snapshots to allow for point-in-time recovery. Secure your Amazon RDS and Amazon Redshift databases using a combination of IAM, database-level access control, network-level access control, and data encryption techniques.

Amazon DynamoDB simplifies the administration and operations of a NoSQL database in the cloud. Amazon DynamoDB allows you to create tables quickly that can scale to an unlimited number of items and configure very high levels of provisioned read and write capacity.

Amazon DynamoDB tables provide a flexible data storage mechanism that only requires a primary key and allows for one or more attributes. Amazon DynamoDB supports both simple scalar data types like String and Number, and also more complex structures using List and Map. Secure your Amazon DynamoDB tables using IAM and restrict access to items and attributes using fine-grained access control.

Amazon DynamoDB will handle the difficult task of cluster and partition management and provide you with a highly available database table that replicates data across Availability Zones for increased durability. Track and process recent changes by tapping into Amazon DynamoDB Streams.

### Exam Essentials

**Know what a relational database is.**
A relational database consists of one or more tables. Communication to and from relational databases usually involves simple SQL queries, such as “Add a new record,” or “What is the cost of product x?” These simple queries are often referred to as OLTP.

**Understand which databases are supported by Amazon RDS.**
Amazon RDS currently supports six relational database engines:

- Microsoft SQL Server
- MySQL Server
- Oracle
- PostgreSQL
- MariaDB
- Amazon Aurora

**Understand the operational benefits of using Amazon RDS.**
Amazon RDS is a managed service provided by AWS. AWS is responsible for patching, antivirus, and management of the underlying guest OS for Amazon RDS. Amazon RDS greatly simplifies the process of setting a secondary slave with replication for failover and setting up read replicas to offload queries.

**Remember that you cannot access the underlying OS for Amazon RDS DB instances.**
You cannot use Remote Desktop Protocol (RDP) or SSH to connect to the underlying OS. If you need to access the OS, install custom software or agents, or want to use a database engine not supported by Amazon RDS, consider running your database on Amazon EC2 instead.

**Know that you can increase availability using Amazon RDS Multi-AZ deployment.**
Add fault tolerance to your Amazon RDS database using Multi-AZ deployment. You can quickly set up a secondary DB Instance in another Availability Zone with Multi-AZ for rapid failover.

**Understand the importance of RPO and RTO.**
Each application should set RPO and RTO targets to define the amount of acceptable data loss and also the amount of time required to recover from an incident. Amazon RDS can be used to meet a wide range of RPO and RTO requirements.

**Understand that Amazon RDS handles Multi-AZ failover for you.**
If your primary Amazon RDS Instance becomes unavailable, AWS fails over to your secondary instance in another Availability Zone automatically. This failover is done by pointing your existing database endpoint to a new IP address. You do not have to change the connection string manually; AWS handles the DNS change automatically.

**Remember that Amazon RDS read replicas are used for scaling out and increased performance.**
This replication feature makes it easy to scale out your read-intensive databases. Read replicas are currently supported in Amazon RDS for MySQL, PostgreSQL, and Amazon Aurora. You can create one or more replicas of a database within a single AWS Region or across multiple AWS Regions. Amazon RDS uses native replication to propagate changes made to a source DB Instance to any associated read replicas. Amazon RDS also supports cross-region read replicas to replicate changes asynchronously to another geography or AWS Region.

**Know what a NoSQL database is.**
NoSQL databases are non-relational databases, meaning that you do not have to have an existing table created in which to store your data. NoSQL databases come in the following formats:

- Document databases
- Graph stores
- Key/value stores
- Wide-column stores

**Remember that Amazon DynamoDB is AWS NoSQL service.**
You should remember that for NoSQL databases, AWS provides a fully managed service called Amazon DynamoDB. Amazon DynamoDB is an extremely fast NoSQL database with predictable performance and high scalability. You can use Amazon DynamoDB to create a table that can store and retrieve any amount of data and serve any level of request traffic. Amazon DynamoDB automatically spreads the data and traffic for the table over a sufficient number of partitions to handle the request capacity specified by the customer and the amount of data stored, while maintaining consistent and fast performance.

**Know what a data warehouse is.**
A data warehouse is a central repository for data that can come from one or more sources. This data repository would be used for query and analysis using OLAP. An organization’s management typically uses a data warehouse to compile reports on specific data. Data warehouses are usually queried with highly complex queries.

**Remember that Amazon Redshift is AWS data warehouse service.**
You should remember that Amazon Redshift is Amazon’s data warehouse service. Amazon Redshift organizes the data by column instead of storing data as a series of rows. Because only the columns involved in the queries are processed and columnar data is stored sequentially on the storage media, column-based systems require far fewer I/Os, which greatly improves query performance. Another advantage of columnar data storage is the increased compression, which can further reduce overall I/O.

### Exercises

1. Create a MySQL Amazon RDS Instance
1. Simulate a Failover from One AZ to Another
1. Create a Read Replica
1. Read and Write from a DynamoDB Table
1. Launch a Redshift Cluster

### Review Questions

- OLTP
- read replicas
- OLAP
- DB snapshot
- Multi-AZ
- failover
- Microsoft SQL Server
- concurrent users
- storage type
- RPO and RTO
- DynamoDB
- Redshift

## Chapter 8: SQS, SWF, and SNS

### Summary

In this chapter, you learned about the core application and mobile services that you will be tested on in your AWS Certified Solutions Architect – Associate exam.

Amazon SQS is a unique service designed by Amazon to help you decouple your infrastructure. Using Amazon SQS, you can store messages on reliable and scalable infrastructure as they travel between distributed components of your applications that perform different tasks, without losing messages or requiring each component to be continuously available.

Understand Amazon SQS queue operations, unique IDs, and metadata. Be familiar with queue and message identifiers such as queue URLs, message IDs, and receipt handles. Understand related concepts such as delay queues, message attributes, long polling, message timers, dead letter queues, access control, and the overall message lifecycle.

Amazon SWF allows you to create applications that coordinate work across distributed components. Amazon SWF is driven by tasks, which are logical units of work that different components of your application perform. To manage tasks across your application, you need to be aware of inter-task dependencies, scheduling of tasks, and using tasks concurrently. Amazon SWF simplifies the coordination of workflow tasks, giving you full control over their implementation without worrying about underlying complexities such as tracking their progress and maintaining their state.

You must be familiar with the following Amazon SWF components and the lifecycle of a workflow execution:

- Workers, starters, and deciders
- Workflows
- Workflow history
- Actors
- Tasks
- Domains
- Object identifiers
- Task lists
- Workflow execution closure
- Long polling

Amazon SNS is a push notification service that lets you send individual or multiple messages to large numbers of recipients. Amazon SNS consists of two types of clients: publishers and subscribers (sometimes known as producers and consumers). Publishers communicate to subscribers asynchronously by sending a message to a topic. A topic is simply a logical access point/communication channel that contains a list of subscribers and the methods used to communicate to them. When you send a message to a topic, it is automatically forwarded to each subscriber of that topic using the communication method configured for that subscriber.

Amazon SNS can support a wide variety of needs, including monitoring applications, workflow systems, time-sensitive information updates, mobile applications, and any other application that generates or consumes notifications. Understand some common Amazon SNS scenarios, including:

- Fanout
- Application and system alerts
- Push email and text messaging
- Mobile push notifications

### Exam Essentials

**Know how to use Amazon SQS.**
Amazon SQS is a unique service designed by Amazon to help you to decouple your infrastructure. Using Amazon SQS, you can store messages on reliable and scalable infrastructure as they travel between your servers. This allows you to move data between distributed components of your applications that perform different tasks without losing messages or requiring each component always to be available.

**Understand Amazon SQS visibility timeouts.**
Visibility timeout is a period of time during which Amazon SQS prevents other components from receiving and processing a message because another component is already processing it. By default, the message visibility timeout is set to **30** seconds, and the maximum that it can be is **12** hours.

**Know how to use Amazon SQS long polling.**
Long polling allows your Amazon SQS client to poll an Amazon SQS queue. If nothing is there, ReceiveMessage waits between **1 and 20 seconds**. If a message arrives in that time, it is returned to the caller as soon as possible. If a message does not arrive in that time, you need to execute the ReceiveMessage function again. This helps you avoid polling in tight loops and prevents you from burning through CPU cycles, keeping costs low.

**Know how to use Amazon SWF.**
Amazon SWF allows you to make applications that coordinate work across distributed components. Amazon SWF is driven by tasks, which are logical units of work that part of your application performs. To manage tasks across your application, you need to be aware of inter-task dependencies, scheduling of tasks, and using tasks concurrently. This is where Amazon SWF can help you. It gives you full control over implementing tasks and coordinating them without worrying about underlying complexities such as tracking their progress and maintaining their state.

**Know the basics of an Amazon SWF workflow.**
A workflow is a collection of activities (coordinated by logic) that carry out a specific goal. For example, a workflow receives a customer order and takes whatever actions are necessary to fulfill it. Each workflow runs in an AWS resource called a domain, which controls the scope of the workflow. An AWS account can have multiple domains, each of which can contain multiple workflows, but workflows in different domains cannot interact.

**Understand the different Amazon SWF actors.**
Amazon SWF interacts with a number of different types of programmatic actors. Actors can be activity workers, workflow starters, or deciders.

**Understand Amazon SNS basics.**
Amazon SNS is a push notification service that lets you send individual or multiple messages to large numbers of recipients. Amazon SNS consists of two types of clients: publishers and subscribers (sometimes known as producers and consumers). Publishers communicate to subscribers asynchronously by sending a message to a topic.

**Know the different protocols used with Amazon SNS.**
You can use the following protocols with Amazon SNS: HTTP, HTTPS, SMS, email, email-JSON, Amazon SQS, and AWS Lambda.

### Exercises

1. Create an Amazon SNS Topic
1. Create a Subscription to Your Topic
1. Publish to a Topic
1. Create Queue
1. Subscribe Queue to SNS Topic

### Review Questions

- different protocols used with Amazon SNS
- features of SNS
- visibility timeout
- actors
- fanout
- message retention period

## Chapter 9: Domain Name System (DNS) and Amazon Route 53

### Summary

In this chapter, you learned the fundamentals of DNS, which is the methodology that computers use to convert human-friendly domain names (for example, amazon.com) into IP addresses (such as 192.0.2.1).

DNS starts with TLDs (for example, .com, .edu). The Internet Assigned Numbers Authority (IANA) controls the TLDs in a root zone database, which is essentially a database of all available TLDs.

DNS names are registered with a domain registrar. A registrar is an authority that can assign domain names directly under one or more TLDs. These domains are registered with InterNIC, a service of ICANN, which enforces the uniqueness of domain names across the Internet. Each domain name becomes registered in a central database, known as the WhoIS database.

DNS consists of a number of different record types, including but not limited to the following:

- A
- AAAA
- CNAME
- MX
- NS
- PTR
- SOA
- SPF
- TXT

Amazon Route 53 is a highly available and highly scalable AWS-provided DNS service. Amazon Route 53 connects user requests to infrastructure running on AWS (for example, Amazon EC2 instances and Elastic Load Balancing load balancers). It can also be used to route users to infrastructure outside of AWS.

With Amazon Route 53, your DNS records are organized into hosted zones that you configure with the Amazon Route 53 API. A hosted zone simply stores records for your domain. These records can consist of A, CNAME, MX, and other supported record types.

Amazon Route 53 allows you to have several different routing policies, including the following:

- **Simple** — Most commonly used when you have a **single** resource that performs a given function for your domain
- **Weighted** — Used when you want to route a **percentage** of your traffic to one particular resource or resources
- **Latency-Based** — Used to route your traffic based on the **lowest latency** so that your users get the fastest response times
- **Failover** — Used for DR and to route your traffic from your resources in a primary location to a **standby** location
- **Geolocation** — Used to route your traffic based on your **end user’s location**

Remember to pull these concepts together to build an application that is highly available and resilient to failures. Use Elastic Load Balancing load balancers across Availability Zones with connection draining enabled, use health checks defined to ensure that the application delegates requests only to healthy Amazon EC2 instances, and use a latency-based routing policy with Elastic Load Balancing health checks to ensure requests are routed with minimal latency to clients. Use Amazon CloudFront edge locations to spread content all over the world with minimal client latency. Deploy the application in multiple AWS regions, protecting it from a regional outage.

### Exam Essentials

**Understand what DNS is.**
DNS is the methodology that computers use to convert human-friendly domain names (for example, amazon.com) into IP addresses (such as 192.0.2.1).

**Know how DNS registration works.**
Domains are registered with domain registrars that in turn register the domain name with InterNIC, a service of ICANN. ICANN enforces uniqueness of domain names across the Internet. Each domain name becomes registered in a central database known as the WhoIS database. Domains are defined by their TLDs. TLDs are controlled by IANA in a root zone database, which is essentially a database of all available TLDs.

**Remember the steps involved in DNS resolution.**
Your browser asks the resolving DNS server what the IP address is for amazon.com. The resolving server does not know the address, so it asks a root server the same question. There are 13 root servers around the world, and these are managed by ICANN. The root server replies that it does not know the answer to this, but it can give an address to a TLD server that knows about .com domain names. The resolving server then contacts the TLD server. The TLD server does not know the address of the domain name either, but it does know the address of the resolving name server. The resolving server then queries the resolving name server. The resolving name server contains the authoritative records and sends these to the resolving server, which then saves these records locally so it does not have to perform these steps again in the near future. The resolving name server returns this information to the user’s web browser, which also caches the information.

**Remember the different record types.**
DNS consists of the following different record types:

- A (address record)
- AAAA (IPv6 address record)
- CNAME (canonical name record or alias)
- MX (mail exchange record)
- NS (name server record)
- PTR (pointer record)
- SOA (start of authority record)
- SPF (sender policy framework)
- SRV (service locator)
- TXT (text record)

**Remember the different routing policies.**
With Amazon Route 53, you can have different routing policies.

- The simple routing policy is most commonly used when you have a single resource that performs a given function for your domain.
- Weighted routing is used when you want to route a percentage of your traffic to a particular resource or resources.
- Latency-based routing is used to route your traffic based on the lowest latency so that your users get the fastest response times.
- Failover routing is used for DR and to route your traffic from a primary resource to a standby resource.
- Geolocation routing is used to route your traffic based on your end user’s location.

### Exercises

1. Create a New Zone
1. Create Two Web Servers in Two Different Regions
1. Create an Alias A Record with a Simple Routing Policy
1. Create a Weighted Routing Policy
1. Create a Hosted Zone for Amazon Virtual Private Cloud (Amazon VPC)

References:

- [Amazon Route 53](http://aws.amazon.com/route53/)

### Review Questions

- type of record
- register a domain name
- routing policy
- DNS port
- DNS protocol
- hosted zones

## Chapter 10: Amazon ElastiCache

### Summary

In this chapter, you learned about caching environments within the cloud using Amazon ElastiCache. You can quickly launch clusters running Memcached or Redis to store frequently used data in-memory. Caching can speed up the response time of your applications, reduce load on your back-end data stores, and improve the user experience.

With Amazon ElastiCache, you can offload the administrative tasks for provisioning and operating clusters and focus on the application. Each cache cluster contains one or more nodes. Select from a range of node types to give the right mix of compute and memory resources for your use case.

You can expand both Memcached and Redis clusters vertically by selecting a larger or smaller node type to match your needs. With Amazon ElastiCache and the Memcached engine, you can also scale your cluster horizontally by adding or removing nodes. With Amazon ElastiCache and the Redis engine, you can also scale horizontally by creating a replication group that will automatically replicate across multiple read replicas.

Streamline your backup and recovery process for Redis clusters with Amazon ElastiCache’s consistent operational model. While Memcached clusters are in-memory only and cannot be persisted, Redis clusters support both automated and manual snapshots. A snapshot can then be restored to recover from a failure or to clone an environment.

You can secure your cache environments at the network level with security groups and network ACLs, and at the infrastructure level using IAM policies. Security groups will serve as your primary access control mechanism to restrict inbound access for active clusters.

You should analyze your data usage patterns and identify frequently run queries or other expensive operations that could be candidates for caching. You can relieve pressure from your database by offloading read requests to the cache tier. Data elements that are accessed on every page load, or with every request but do not change, are often prime candidates for caching. Even data that changes frequently can often benefit from being cached with very large request volumes.

### Exam Essentials

**Know how to use Amazon ElastiCache.**
Improve the performance of your application by deploying Amazon ElastiCache clusters as part of your application and offloading read requests for frequently accessed data. Use the cache-aside pattern in your application first to check the cache for your query results before checking the database.

**Understand when to use a specific cache engine.**
Amazon ElastiCache gives you the choice of cache engine to suit your requirements.

- Use Memcached when you need a simple, in-memory object store that can be easily partitioned and scaled horizontally.
- Use Redis when you need to back up and restore your data, need many clones or read replicas, or are looking for advanced functionality like sort and rank or leaderboards that Redis natively supports.

**Understand how to scale a Redis cluster horizontally.**
An Amazon ElastiCache cluster running Redis can be scaled horizontally first by creating a replication group, then by creating additional clusters and adding them to the replication group.

**Understand how to scale a Memcached cluster horizontally.**
An Amazon ElastiCache cluster running Memcached can be scaled horizontally by adding or removing additional cache nodes to the cluster. The Amazon ElastiCache client library supports Auto Discovery and can discover new nodes added or removed from the cluster without having to hardcode the list of nodes.

**Know how to back up your Amazon ElastiCache cluster.**
You can create a snapshot to back up your Amazon ElastiCache clusters running the Redis engine. Snapshots can be created automatically on a daily basis or manually on demand. Amazon ElastiCache clusters running Memcached do not support backup and restore natively.

### Exercises

1. Create an Amazon ElastiCache Cluster Running Memcached
1. Expand the Size of a Memcached Cluster
1. Create an Amazon ElastiCache Cluster and Redis Replication Group

### Review Questions

- number of nodes
- cache engines

## Chapter 11: Additional Key Services

### Summary

In this chapter, you learned about additional key AWS cloud services, many of which will be covered on your AWS Certified Solutions Architect – Associate exam. These services are grouped into **four** categories of services: storage and content delivery, security, analytics, and DevOps.

In the **storage and content delivery** group, we covered Amazon CloudFront and AWS Storage Gateway.

- **Amazon CloudFront** is a global CDN service. It integrates with other AWS products to give developers and businesses an easy way to distribute content to end users with low latency, high data transfer speeds, and no minimum usage commitments.
- **AWS Storage Gateway** is a service that connects an on-premises software appliance with cloud-based storage. It provides seamless and secure integration between an organization’s on-premises IT environment and AWS storage infrastructure. The AWS Storage Gateway appliance maintains frequently accessed data on-premises while encrypting and storing all of your data in Amazon S3 or Amazon Glacier.

The services we covered in **security** focused on Identity Management (AWS Directory Service), Key Management (AWS KMS AWS CloudHSM), and Audit (AWS CloudTrail).

- **AWS Directory Service** is a managed service offering, providing directories that contain information about your organization, including users, groups, computers, and other resources. AWS Directory Service is offered in three types: AWS Directory Service for Microsoft Active Directory (Enterprise Edition), Simple AD, and AD Connector.
- **Key management** is the management of cryptographic keys within a cryptosystem. This includes dealing with the generation, exchange, storage, use, and replacement of keys.
  - AWS **KMS** is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. AWS KMS lets you create keys that can never be exported from the service and that can be used to encrypt and decrypt data based on policies you define.
  - AWS **CloudHSM** helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated HSM appliances within the AWS cloud. An HSM is a hardware appliance that provides secure key storage and cryptographic operations within a tamper-resistant hardware module.
- AWS **CloudTrail** provides visibility into user activity by recording API calls made on your account. AWS CloudTrail records important information about each API call, including the name of the API, the identity of the caller, the time of the API call, the request parameters, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.

The **analytics** services covered help you overcome the unique list of challenges associated with big data in today’s IT world.

- **Amazon Kinesis** is a platform for handling massive streaming data on AWS, offering powerful services to make it easy to load and analyze streaming data and also providing the ability for you to build custom streaming data applications for specialized needs.
- **Amazon EMR** provides you with a fully managed, on-demand Hadoop framework. The reduction of complexity and up-front costs combined with the scale of AWS means you can instantly spin up large Hadoop clusters and start processing within minutes.

To supplement the big data challenges, orchestrating data movement comes with its own challenges.

- **AWS Data Pipeline** is a web service that helps you reliably process and move data between different AWS compute and storage services, and also on-premises data sources, at specified intervals. With AWS Data Pipeline, you can regularly access your data where it’s stored, transform and process it at scale, and efficiently transfer the results to AWS services such as Amazon S3, Amazon RDS, Amazon DynamoDB, and Amazon EMR.
- **AWS Import/Export** helps when you’re faced with the challenge of getting huge datasets into AWS in the first place or retrieving them back to on-premises when necessary. AWS Import/Export is a service that accelerates transferring large amounts of data into and out of AWS using physical storage appliances, bypassing the Internet. The data is copied to a device at the source, shipped via standard shipping mechanisms, and then copied to the destination.

AWS continues to evolve services in support of organizations embracing **DevOps**. Services such as AWS OpsWorks, AWS CloudFormation, AWS Elastic Beanstalk, and AWS Config are leading the way for DevOps on AWS.

- AWS **OpsWorks** provides a configuration management service that helps you configure and operate applications using Chef. AWS OpsWorks works with applications of any level of complexity and is independent of any particular architectural pattern.
- AWS **CloudFormation** allows organizations to deploy, modify, and update resources in a controlled and predictable way, in effect applying version control to AWS infrastructure the same way one would do with software.
- AWS **Elastic Beanstalk** allows developers to simply upload their application code, and the service automatically handles all of the details such as resource provisioning, load balancing, Auto Scaling, and monitoring.
- **AWS Config** delivers a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config, organizations have the information necessary for compliance auditing, security analysis, resource change tracking, and troubleshooting.

### Exam Essentials

**Know the basic use cases for amazon CloudFront.**
Know **when to use** Amazon CloudFront (for popular static and dynamic content with geographically distributed users) and **when not to** (all users at a single location or connecting through a corporate VPN).

**Know how amazon CloudFront works.**
Amazon CloudFront optimizes downloads by using geolocation to identify the geographical location of users, then serving and caching content at the edge location closest to each user to maximize performance.

**Know how to create an amazon CloudFront distribution and what types of origins are supported.**
To create a distribution, you specify an origin and the type of distribution, and Amazon CloudFront creates a new domain name for the distribution. Origins supported include Amazon S3 buckets or static Amazon S3 websites and HTTP servers located in Amazon EC2 or in your own data center.

**Know how to use amazon CloudFront for dynamic content and multiple origins.**
Understand how to specify multiple origins for different types of content and how to use cache behaviors and path strings to control what content is served by which origin.

**Know what mechanisms are available to serve private content through amazon CloudFront.**
Amazon CloudFront can serve private content using Amazon S3 Origin Access Identifiers, signed URLs, and signed cookies.

**Know the three configurations of AWS storage gateway and their use cases.**

- **Gateway-Cached** volumes expand your on-premises storage into Amazon S3 and cache frequently used files locally.
- **Gateway-Stored** values keep all your data available locally at all times and also replicate it asynchronously to Amazon S3.
- **Gateway-VTL** enables you to keep your current backup tape software and processes while eliminating physical tapes by storing your data in the cloud.

**Understand the value of AWS Directory Service.**
AWS Directory Service is designed to reduce identity management tasks, thereby allowing you to focus more of your time and resources on your business.

**Know the AWS Directory Service Directory types.**
AWS Directory Service offers three directory types:

- AWS Directory Service for Microsoft Active Directory (Enterprise Edition), also referred to as Microsoft AD
- Simple AD
- AD Connector

**Know when you should use AWS Directory Service for Microsoft Active Directory.**
You should use Microsoft Active Directory if you have more than 5,000 users or need a trust relationship set up between an AWS hosted directory and your on-premises directories.

**Understand key management.**
Key management is the management of cryptographic keys within a cryptosystem. This includes dealing with the generation, exchange, storage, use, and replacement of keys.

**Understand when you should use AWS KMS.**
AWS KMS is a managed service that makes it easy for you to create and control the symmetric encryption keys used to encrypt your data. AWS KMS lets you create keys that can never be exported from the service and which can be used to encrypt and decrypt data based on policies you define.

**Understand when you should use AWS CloudHSM.**
AWS CloudHSM helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated hardware security module appliances within the AWS cloud.

**Understand the value of AWS CloudTrail.**
AWS CloudTrail provides visibility into user activity by recording API calls made on your account. This helps you to track changes made to your AWS resources and to troubleshoot operational issues. AWS CloudTrail makes it easier to ensure compliance with internal policies and regulatory standards.

**Know the three services of Amazon kinesis and their use cases.**
Amazon Kinesis Firehose allows you to load massive volumes of streaming data into AWS. Amazon Kinesis Analytics enables you to easily analyze streaming data real time with standard SQL. Amazon Kinesis Streams enables you to build custom applications that process or analyze streaming data real time for specialized needs.

**Know what service Amazon EMR provides.**
Amazon EMR provides a managed Hadoop service on AWS that allows you to spin up large Hadoop clusters in minutes.

**Know the difference between persistent and transient clusters.**
Persistent clusters run continuously, so they do not lose data stored on instance-based HDFS. Transient clusters are launched for a specific task, then terminated, so they access data on Amazon S3 via EMRFS.

**Know the use cases for Amazon EMR.**
Amazon EMR is useful for big data analytics in virtually any industry, including, but not limited to, log processing, clickstream analysis, and genomics and life sciences.

**Know the use cases for AWS data pipeline.**
AWS Data Pipeline can manage batch ETL processes at scale on the cloud, accessing data both in AWS and on-premises. It can take advantage of AWS cloud services by spinning up resources required for the process, such as Amazon EC2 instances or Amazon EMR clusters.

**Know the types of AWS import/export services and the possible sources/destinations of each.**
AWS Snowball is Amazon shippable appliances supplied ready to ship. It can transfer data to and from your on-premises storage and to and from Amazon S3. AWS Import/Export Disk uses your storage devices and, in addition to transferring data in and out of your on-premises storage, can import data to Amazon S3, Amazon EBS, and Amazon S3; it can only export data from Amazon S3.

**Understand the basics of AWS opsworks.**
AWS OpsWorks is a configuration management service that helps you configure and operate applications of all shapes and sizes using Chef. You can define an application’s architecture and the specification of each component including package installation, software configuration, and resources such as storage.

**Understand the value of AWS cloudformation.**
AWS CloudFormation is a service that helps you model and set up your AWS resources. AWS CloudFormation allows organizations to deploy, modify, and update resources in a controlled and predictable way, in effect applying version control to AWS infrastructure the same way you would do with software.

**Understand the value of AWS elastic beanstalk.**
AWS Elastic Beanstalk is the fastest and simplest way to get an application up and running on AWS. Developers can simply upload their application code, and the service automatically handles all the details such as resource provisioning, load balancing, Auto Scaling, and monitoring.

**Understand the components of AWS elastic beanstalk.**
An AWS Elastic Beanstalk application is the logical collection of environments, versions, and environment configurations. In AWS Elastic Beanstalk, an application is conceptually similar to a folder.

**Understand the value of AWS config.**
AWS Config is a fully managed service that provides organizations with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config, organizations can discover existing and deleted AWS resources, determine their overall compliance against rules and dive into configuration details of a resource at any point in time. These capabilities enable compliance auditing, security analysis, resource change tracking, and troubleshooting.

## Review Questions

## Chapter 12: Security on AWS

### Exam Essentials

**Understand the shared responsibility model.**
AWS is responsible for securing the underlying infrastructure that supports the cloud, and you’re responsible for anything you put on the cloud or connect to the cloud.

**Understand regions and Availability Zones.**
Each region is completely independent. Each region is designed to be completely isolated from the other regions. This achieves the greatest possible fault tolerance and stability. Regions are a collection of Availability Zones. Each Availability Zone is isolated, but the Availability Zones in a region are connected through low-latency links.

**Understand High-Availability System Design within AWS.**
You should architect your AWS usage to take advantage of multiple regions and Availability Zones. Distributing applications across multiple Availability Zones provides the ability to remain resilient in the face of most failure modes, including natural disasters or system failures.

**Understand the network security of AWS.**
Network devices, including firewall and other boundary devices, are in place to monitor and control communications at the external boundary of the network and at key internal boundaries within the network. These boundary devices employ rule sets, ACLs, and configurations to enforce the flow of information to specific information system services.

AWS has strategically placed a limited number of access points to the cloud to allow for a more comprehensive monitoring of inbound and outbound communications and network traffic.

These customer access points are called API endpoints, and they allow HTTPS access, which allows you to establish a secure communication session with your storage or compute instances within AWS.

**Amazon EC2 instances cannot send spoofed network traffic.**
The AWS-controlled, host-based firewall infrastructure will not permit an instance to send traffic with a source IP or MAC address other than its own.

**Unauthorized port scans by Amazon EC2 customers are a violation of the AWS Acceptable Use Policy.**
Violations of the AWS Acceptable Use Policy are taken seriously, and every reported violation is investigated.

It is not possible for an Amazon EC2 instance running in promiscuous mode to receive or “sniff” traffic that is intended for a different virtual instance.

**Understand the use of credentials on AWS.**
AWS employs several credentials in order to positively identify a person or authorize an API call to the platform. Credentials include:

- Passwords
- AWS root account or IAM user account login to the AWS Management Console
- Multi-Factor Authentication (MFA)
- AWS root account or IAM user account login to the AWS Management Console
- Access Keys
- Digitally signed requests to AWS APIs (using the AWS SDK, CLI, or REST/Query APIs)

**Understand the proper use of access keys.**
Because access keys can be misused if they fall into the wrong hands, AWS encourages you to save them in a safe place and not to embed them in your code. For customers with large fleets of elastically-scaling Amazon EC2 instances, the use of IAM roles can be a more secure and convenient way to manage the distribution of access keys.

**Understand the value of AWS CloudTrail.**
AWS CloudTrail is a web service that records API calls made on your account and delivers log files to your Amazon S3 bucket. AWS CloudTrail’s benefit is visibility into account activity by recording API calls made on your account.

**Understand the security features of Amazon EC2.**
Amazon EC2 uses public-key cryptography to encrypt and decrypt login information. Public-key cryptography uses a public key to encrypt a piece of data, such as a password, and then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair.

To log in to your instance, you must create a key pair, specify the name of the key pair when you launch the instance, and provide the private key when you connect to the instance. Linux instances have no password, and you use a key pair to log in using SSH. With Windows instances, you use a key pair to obtain the administrator password and then log in using RDP.

A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you associate one or more security groups with the instance. You add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group.

**Understand AWS use of encryption of data in transit.**
All service endpoints support encryption of data in transit via HTTPS.

**Know which services offer encryption of data at rest as a feature.**
The following services offer a feature to encrypt data at rest:

- Amazon S3
- Amazon EBS
- Amazon Glacier
- AWS Storage Gateway
- Amazon RDS
- Amazon Redshift
- Amazon WorkSpaces

## Chapter 13: AWS Risk and Compliance

### Summary

AWS communicates with customers regarding its security and control environment through the following mechanisms:

- Obtaining industry certifications and independent third-party attestations
- Publishing information about security and AWS control practices via the website, whitepapers, and blogs
- Directly providing customers with certificates, reports, and other documentation (under NDA in some cases)\*

The shared responsibility model is not just limited to security considerations; it also extends to IT controls. The management, operation, and verification of IT controls are shared between AWS and the customer. AWS manages these controls where it relates to the physical infrastructure, and the customer manages these controls for the guest operating systems and upward (depending on the service).

It is the customer’s responsibility to maintain adequate governance over the entire IT control environment, regardless of how their IT is deployed (on-premises, cloud, or hybrid). By deploying to the AWS Cloud, customers have different options for applying different types of controls and various verification methods that align with their business requirements.

The control environment for AWS contains a large volume of information. This information is provided to customers through whitepapers, reports, certifications, and other third-party attestations. AWS provides IT control information to customers in two ways: specific control definition and general control standard compliance.

AWS provides documentation about its risk and compliance program. This documentation can enable customers to include AWS controls in their governance frameworks. The three core areas of the risk and compliance program are risk management, control environment, and information security.

AWS has achieved a number of internationally recognized certifications and accreditations that demonstrate AWS compliance with third-party assurance frameworks, including:

- FedRAMP
- FIPS 140–2
- FISMA and DIACAP
- HIPAA
- ISO 9001
- ISO 27001
- ITAR
- PCI DSS Level 1
- SOC 1/ISAE 3402
- SOC 2
- SOC 3

AWS is constantly listening to customers and examining other certifications for the future.

### Exam Essentials

**Understand the shared responsibility model.**
The shared responsibility model is not just limited to security considerations; it also extends to IT controls. For example, the management, operation, and verification of IT controls are shared between AWS and the customer. AWS manages these controls where it relates to physical infrastructure.

**Remember that IT governance is the customer’s responsibility.**
It is the customer’s responsibility to maintain adequate governance over the entire IT control environment, regardless of how its IT is deployed (on-premises, cloud, or hybrid).

**Understand how AWS provides control information.**
AWS provides IT control information to customers in two ways: via specific control definition and through a more general control standard compliance.

**Remember that AWS is very proactive about risk management.**
AWS takes risk management very seriously, so it has developed a business plan to identify any risks and to implement controls to mitigate or manage those risks. An AWS management team reevaluates the business risk plan at least twice a year. As a part of this process, management team members are required to identify risks within their specific areas of responsibility and then implement controls designed to address and perhaps even eliminate those risks.

**Remember that the control environment is not just about technology.**
The AWS control environment consists of policies, processes, and control activities. This control environment includes people, processes, and technology.

**Remember the key reports, certifications, and third-party attestations.**
The key reports, certifications, and third-party attestations include, but are not limited to, the following:

- FedRAMP
- FIPS 140–2
- FISMA and DIACAP
- HIPAA
- ISO 9001
- ISO 27001
- ITAR
- PCI DSS Level 1
- SOC 1/ISAE 3402
- SOC 2
- SOC 3

## Chapter 14: Architecture Best Practices

### Summary

Typically, production systems come with defined or implicit requirements in terms of uptime. A system is highly available when it can withstand the failure of an individual or multiple components. If you design architectures around the assumption that any component will eventually fail, systems won’t fail when an individual component does.

Traditional infrastructure generally necessitates predicting the amount of computing resources your application will use over a period of several years. If you underestimate, your applications will not have the horsepower to handle unexpected traffic, potentially resulting in customer dissatisfaction. If you overestimate, you’re wasting money with superfluous resources. The on-demand and elastic nature of the cloud enables the infrastructure to be closely aligned with the actual demand, thereby increasing overall utilization and reducing cost. While cloud computing provides virtually unlimited on-demand capacity, system architectures need to be able to take advantage of those resources seamlessly. There are generally two ways to scale an IT architecture: vertically and horizontally.

The AWS Cloud provides governance capabilities that enable continuous monitoring of configuration changes to your IT resources. Because AWS assets are programmable resources, your security policy can be formalized and embedded with the design of your infrastructure. With the ability to spin up temporary environments, security testing can now become part of your continuous delivery pipeline. Solutions Architects can leverage a plethora of native AWS security and encryption features that can help achieve higher levels of data protection and compliance at every layer of cloud architectures.

Because AWS makes parallelization effortless, Solutions Architects need to internalize the concept of parallelization when designing architectures in the cloud. It is advisable not only to implement parallelization wherever possible, but also to automate it because the cloud allows you to create a repeatable process very easily.

As application complexity increases, a desirable characteristic of an IT system is that it can be broken into smaller, loosely coupled components. Solutions Architects should design systems in a way that reduces interdependencies, so that a change or a failure in one component does not cascade to other components.

When organizations try to map their existing system specifications to those available in the cloud, they notice that the cloud might not have the exact specification of the resource that they have on-premises. Organizations should not be afraid and feel constrained when using cloud resources. Even if you might not get an exact replica of your hardware in the cloud environment, you have the ability to get more of those resources in the cloud to compensate.

By focusing on concepts and best practices—like designing for failure, decoupling the application components, understanding and implementing elasticity, combining it with parallelization, and integrating security in every aspect of the application architecture—Solutions Architects can understand the design considerations necessary for building highly scalable cloud applications.

As each use case is unique, Solutions Architects need to remain diligent in evaluating how best practices and patterns can be applied to each implementation. The topic of cloud computing architectures is broad and continuously evolving.

### Exam Essentials

**Understand highly available architectures.**
A system is highly available when it can withstand the failure of an individual or multiple components. If you design architectures around the assumption that any component will eventually fail, systems won’t fail when an individual component does.

**Understand redundancy.**
Redundancy can be implemented in either standby or active mode. When a resource fails in standby redundancy, functionality is recovered on a secondary resource using a process called failover. The failover will typically require some time before it is completed, and during that period the resource remains unavailable. In active redundancy, requests are distributed to multiple redundant compute resources, and when one of them fails, the rest can simply absorb a larger share of the workload. Compared to standby redundancy, active redundancy can achieve better utilization and affect a smaller population when there is a failure.

**Understand elasticity.**
Elastic architectures can support growth in users, traffic, or data size with no drop in performance. It is important to build elastic systems on top of a scalable architecture. These architectures should scale in a linear manner, where adding extra resources results in at least a proportional increase in ability to serve additional system load. The growth in resources should introduce economies of scale, and cost should follow the same dimension that generates business value out of that system. There are generally two ways to scale an IT architecture: vertically and horizontally.

**Understand vertical scaling.**
Scaling vertically takes place through an increase in the specifications of an individual resource (for example, upgrading a server with a larger hard drive or a faster CPU). This way of scaling can eventually hit a limit, and it is not always a cost efficient or highly available approach.

**Understand horizontal scaling.**
Scaling horizontally takes place through an increase in the number of resources. This is a great way to build Internet-scale applications that leverage the elasticity of cloud computing. It is important to understand the impact of stateless and stateful architectures before implementing horizontal scaling.

**Understand stateless applications.**
A stateless application needs no knowledge of the previous interactions and stores no session information. A stateless application can scale horizontally because any request can be serviced by any of the available system compute resources.

**Understand loose coupling.**
As application complexity increases, a desirable characteristic of an IT system is that it can be broken into smaller, loosely coupled components. This means that IT systems should be designed as “black boxes” to reduce interdependencies so that a change or a failure in one component does not cascade to other components. The more loosely system components are coupled, the larger they scale.

**Understand the different storage options in AWS.**
AWS offers a broad range of storage choices for backup, archiving, and disaster recovery, as well as block, file, and object storage to suit a plethora of use cases. It is important from a cost, performance, and functional aspect to leverage different storage options available in AWS for different types of datasets.

### Exercises

1. Create a Custom Amazon VPC
1. Create an Internet Gateway for Your Custom Amazon VPC
1. Update the Main Route Table for Your Custom Amazon VPC
1. Create Public Subnets for Your Custom Amazon VPC
1. Create a NAT Gateway for Your Custom Amazon VPC
1. Create a Private Route Table for Your Custom Amazon VPC
1. Create Private Subnets for Your Custom Amazon VPC
1. Create Security Groups for Each Application Tier
1. Create a MySQL Multi-AZ Amazon RDS Instance
1. Create an Elastic Load Balancer (ELB)
1. Create a Web Server Auto Scaling Group
1. Create a Route 53 Hosted Zone
1. Create an Alias A Record
1. Test Your Configuration

## References

1. AWS Certified Solutions Architect Official Study Guide [Book]. https://www.oreilly.com/library/view/aws-certified-solutions/9781119138556/. Accessed 4 Jan. 2021.
